[
  {
    "objectID": "posts/llm-primer/index.html#installs",
    "href": "posts/llm-primer/index.html#installs",
    "title": "LLM Fine-Tuning: A Primer",
    "section": "Installs",
    "text": "Installs\nAll of the packages along with their versions are stated in the Axolotl requirements.txt, we will just need to install two more.\npip install datasets \"huggingface_hub[cli]\""
  },
  {
    "objectID": "posts/llm-primer/index.html#api-keys",
    "href": "posts/llm-primer/index.html#api-keys",
    "title": "LLM Fine-Tuning: A Primer",
    "section": "API Keys",
    "text": "API Keys\nWe’re going to push our trained model and dataset to HuggingFace, and we’ll also track our fine-tuning runs with W&B, so we’re going to need our API keys for each of those.\nFor HF, go to your profile by clicking your avatar at the upper right corner of the screen, Settings -&gt; Access Tokens -&gt; New Token. Create one, give it write permissions and copy it. Then in the terminal run huggingface-cli login, paste your token and hit Enter. To verify that you’re logged in, you can run huggingface-cli whoami.\nFor W&B, head to the authorize page, and copy your key from there."
  },
  {
    "objectID": "posts/llm-primer/index.html#base-model",
    "href": "posts/llm-primer/index.html#base-model",
    "title": "LLM Fine-Tuning: A Primer",
    "section": "Base Model",
    "text": "Base Model\nbase_model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer"
  },
  {
    "objectID": "posts/llm-primer/index.html#dataset",
    "href": "posts/llm-primer/index.html#dataset",
    "title": "LLM Fine-Tuning: A Primer",
    "section": "Dataset",
    "text": "Dataset\ndatasets:\n  - path: &lt;your_hf_username&gt;/databricks-dolly-15k\n    type: alpaca\ndataset_prepared_path:\nval_set_size: 0.05\noutput_dir: ./outputs/lora-out"
  },
  {
    "objectID": "posts/llm-primer/index.html#hf-repo",
    "href": "posts/llm-primer/index.html#hf-repo",
    "title": "LLM Fine-Tuning: A Primer",
    "section": "HF repo",
    "text": "HF repo\nhub_model_id: &lt;username/repo_id&gt;"
  },
  {
    "objectID": "posts/llm-primer/index.html#wb-runs",
    "href": "posts/llm-primer/index.html#wb-runs",
    "title": "LLM Fine-Tuning: A Primer",
    "section": "W&B Runs",
    "text": "W&B Runs\nwandb_project: &lt;project_name&gt;\nwandb_entity: &lt;your_username&gt;\nwandb_name: &lt;optional_run_name&gt;\nNotice how we specify our dataset type as alpaca. This is actually to tell Axolotl to format our dataset rows into the Alpaca prompt template. A prompt template is essentially the “key” that we use to overcome the LLM’s autocomplete nature that it was pretrained on, and adapt it to answer questions or perform tasks. It is just a way to organize an instruction and its response (and optionally its input), into a predetermined format so that the LLM learns; that’s the reason why we renamed our dataset columns earlier to the Alpaca column names."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Documenting my learning journey, (kinda) guides, and some thoughts"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yet Another Blog",
    "section": "",
    "text": "LLM Fine-Tuning: A Primer\n\n\n\n\n\n\nLLM\n\n\nAxolotl\n\n\nCode\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nKareem Amr\n\n\n\n\n\n\nNo matching items"
  }
]